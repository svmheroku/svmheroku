/! views/r10/glossary.html.slim
=render :partial => 'glossary'

h4 Glossary

hr/

a(href="/glossary#bot")
  h4#bot Bot

' I use the term bot to describe software which copies fresh data from the internet at frequent intervals and then processes that data into more valuable data.

br/
br/
' Wikipedia has a definition which matches my idea of a bot:
br/
br/
a(target="w" href="http://en.wikipedia.org/wiki/Internet_bot")http://en.wikipedia.org/wiki/Internet_bot
br/
br/
' This site relies on the "DanBot" which copies fresh data from the internet every 5 minutes and then issues a set of near-future-price-predictions for many stocks and 11 currency pairs:
br/
br/
a(href="/predictions")http://bot4.us/predictions
br/
br/

a(href="/glossary#scatter_plot")
  h4#scatter_plot Scatter-plot

' Wikipedia has a definition which matches my idea of a scatter-plot:
br/
br/
a(target="w" href="http://en.wikipedia.org/wiki/Scatter_plot")http://en.wikipedia.org/wiki/Scatter_plot
br/
br/
' Visualization of a scatter-plot is useful for understanding the SVM Algorithm which is at the heart of the technology behind the DanBot.
br/
br/

' It is easy to see examples of scatter-plots at the URL below:
br/
br/

a(href="http://www.google.com/search?q=scatter+plot" target="g")http://www.google.com/search?q=scatter+plot
br/
br/

' Once you understand that a scatter-plot can be used to convey a dependence of a variable on the vertical axis to a variable on the horizontal axis, then you might ask, "How do I quantify the strength and direction of this dependence?"

br/
br/

' One answer to the above question is a statistical calculation called "Pearson's correlation" which I refer to as just "Correlation".


a(href="/glossary#correlation")
  h4#correlation Correlation

' Wikipedia has an article on Correlation:
br/
br/

a(target="w" href="http://en.wikipedia.org/wiki/Correlation_and_dependence")http://en.wikipedia.org/wiki/Correlation_and_dependence
br/
br/

' When you look at a scatter-plot of many points you can usually detect the strength and direction of their correlation.

br/
br/
' For Earth, if you plotted distance from the equator on the horizontal axis and average temperature on the vertical axis, you would see a cluster of points which leans to the left.
br/
br/
' This scatter-plot describes the fact that as a thermometer moves away from the equator, it records cooler temperatures.
br/
br/
' This situation is an example of negative correlation.
br/
br/

' More examples of negative correlation are displayed at the URL below:
br/
br/

a(href="http://www.google.com/search?q=negative+correlation" target="g")http://www.google.com/search?q=negative+correlation
br/
br/

' If you want an example of positive correlation, create a scatter plot of average human height on the horizontal axis and average human weight on the vertical axis.
br/
br/
' This scatter-plot tells me that taller humans usually weigh more that shorter humans.

br/
br/
' Since the author of the scatter-plot has placed height on the horizontal axis, he/she is expressing the opinion that average weight depends on average height.
br/
br/

' Suppose I want to use a scatter plot to help me predict the gain of IBM-price 24 hours after an initial 24-hour-IBM-price-gain has been measured?

br/
br/
' This would be easy to do.  Every 5 minutes when the market is open I collect the price of IBM, and then I look back in time and collect two prices for IBM.  I get the price for 48 hours ago and the price 24 hours ago.  
' Next, I calculate two gains, an older gain and a newer gain.  
' Then, I create a scatter-plot with the older gain on the horizontal axis.
br/
br/
' If the newer gains depend on the older gains, this might be apparent via visual inspection of the scatter-plot.
' Obviously the gain of IBM-price 24 hours from now is dependent on many other variables.  
br/
br/
' Since Pearson's correlation can only be used with 2 variables, it is not a very powerful tool for predicting future behavior of the price of IBM.  
br/
br/
' What is needed is an algorithm which can analyze many variables at once.

a(href="/glossary#vector")
  h4#vector Vector
' A scatter-plot is made of points.  And each point is made of two variables.  A vector is similar to a point.  A two dimensional vector, like a point, is made from two variables.  A set of two dimensional vectors is easy to visualize.  Just call them points and display them in a scatter-plot.  A vector can be made from 3 variables or any number of variables.  
br/
br/
' Sometimes when I discuss vectors and the variables which make them up, I call the variables "attributes" instead of variables.  So, attribute is a synonym for variable.
br/
br/
' Typically when I specify a vector from attributes, I build it from attributes that are easy to collect or know and then make the last attribute the one which I want to predict.
br/
br/
' Here is an example of such a vector (a four dimensional vector):
ul
  li 24 hour gain of IBM measured 10 hours ago
  li 24 hour gain of IBM measured 5 minutes ago
  li Slope of the 5-day moving average of IBM-price
  li Gain of IBM between now and next 24 hours

' When I try to visualize a set of three dimensional vectors, that is usually more difficult to see.  
' In some situations I might get lucky.  For example if the third attribute in each vector is constrained to be 1 or 2, then I could visualize the set of three dimensional vectors as two scatter-plots.  
' Obviously as I encounter systems which have a large number of large vectors and I want to use these systems to find dependencies between the attributes I can't rely on visualization of a scatter plot to gauge the dependency.  
br/
br/
' Instead, I need to rely on a calculation like Pearson's correlation to help me.
br/
br/
' Unfortunately, the Pearson's correlation calculation assumes we have a set of two dimensional vectors.  
br/
br/
' If we are working with vectors that have 3 or more dimensions and we want to find dependencies between the attributes in the vectors, we need an algorithm which is more powerful than Pearson's correlation calculation.

a(href="/glossary#svm")
  h4#svm Support Vector Machine (SVM)

' Support Vector Machine (SVM) is an algorithm I can use to quantify the dependency between the last attribute in a vector with the other attributes in the vector.
br/
br/
' And once this dependency is quantified I can use SVM to predict the value of the last attribute in a vector when the values of the other attributes are known.
br/
br/
' A link to more discussion about SVM is displayed below:
br/
br/
a(target="w" href="http://en.wikipedia.org/wiki/Support_vector_machine")http://en.wikipedia.org/wiki/Support_vector_machine
br/
br/
' A scatter-plot which helps me visualize SVM at work is displayed below:
br/
br/
img(src="/images/svm3d.png" alt="SVM_at_work")
br/
br/
' It is showing a set of 16 vectors which are each 3 dimensional vectors.  
' The first attribute in each vector corresponds to the x1 axis and the second attribute in each vector corresponds to the x2 axis.  
br/
br/
' The third attribute, x3, in each vector is constrained to the values 0 or 1.  
' If x3 is 1 it is displayed as a black circle else it appears as a white circle.  
br/
br/
' This scatter-plot of three dimensional vectors, describes the general type of problem I want to solve:
br/
br/
' If I know the values of x1 and x2, can I predict the value of x3?
br/
br/
' The scatter-plot shows that when x1 is large and x2 is small, then x3 is a white circle and thus x3 is 0.
br/
br/
' And, when x1 is small and x2 is large, then x3 is a black  circle and thus x3 is 1.
br/
br/
' SVM is designed to calculate the "best" way to measure the separation between the black circles and the white circles.
br/
br/
' The green line shows an attempt by SVM to measure the separation between the black circles and the white circles.
br/
br/
' The blue line also shows an attempt by SVM to measure the separation between the black circles and the white circles.  
' It's obvious from the diagram that the blue line does a better job than the green line.
br/
br/
' Finally, the red line shows an optimal attempt by SVM to measure the separation between the black circles and the white circles.  
br/
br/
' SVM practitioners refer to these lines as hyperplanes.  
br/
br/
' Once an SVM-instance "knows" where the red-hyperplane resides, the algorithm can then make predictions.
br/
br/
' What is missing from the diagram is a vector displayed as a gray circle.  I would color it gray to indicate that x1 and x2 are known but x3 is unknown and that I want to predict it.  
' Once the gray-circle-vector is plotted, I would see if the vector is to the left of the red-hyperplane or to the right of it.  
' If the vector lands on the left, I'd predict that x3 is 1 else I'd predict it is 0.
' SVM does not have eyes like I do.  Instead it would measure both the direction and distance of the gray circle from the red-hyperplane.  It would then use this measurement to predict the value of x3.

br/
br/
hr/
